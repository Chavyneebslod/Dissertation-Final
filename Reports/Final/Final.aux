\relax 
\ifx\hyper@anchor\@undefined
\global \let \oldcontentsline\contentsline
\gdef \contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global \let \oldnewlabel\newlabel
\gdef \newlabel#1#2{\newlabelxx{#1}#2}
\gdef \newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\let \contentsline\oldcontentsline
\let \newlabel\oldnewlabel}
\else
\global \let \hyper@last\relax 
\fi

\citation{stocks}
\citation{nukes}
\citation{Chu36a}
\citation{complexityZoo}
\citation{buss:560}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{5}{section.1}}
\newlabel{sec:intro}{{1}{5}{Introduction\relax }{section.1}{}}
\citation{website:quantum}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Statement of objective}{6}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Definition of NP}{6}{subsection.1.2}}
\newlabel{eq:NPproblems}{{2}{6}{Definition of NP\relax }{equation.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Why NP problems are intractable}{6}{subsection.1.3}}
\newlabel{sec:NPint}{{1.3}{6}{Why NP problems are intractable\relax }{subsection.1.3}{}}
\citation{Sipser:2005}
\citation{Sipser:2005}
\citation{citeulike:Karp}
\citation{citeulike:Cook}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}NP--completeness}{7}{subsection.1.4}}
\newlabel{NPC}{{1.4}{7}{NP--completeness\relax }{subsection.1.4}{}}
\newlabel{eq:NPC}{{3}{7}{NP--completeness\relax }{equation.3}{}}
\citation{citeulike:Karp}
\citation{Sipser:2005}
\@writefile{toc}{\contentsline {section}{\numberline {2}Objectives}{8}{section.2}}
\newlabel{Objectives}{{2}{8}{Objectives\relax }{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Problem}{8}{subsection.2.1}}
\newlabel{sec:prob}{{2.1}{8}{Problem\relax }{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Methods}{8}{subsection.2.2}}
\newlabel{sec:meth}{{2.2}{8}{Methods\relax }{subsection.2.2}{}}
\citation{paper:hybrid}
\citation{SafDinur}
\citation{Khot}
\citation{citeulike:NASA}
\citation{queens}
\citation{BAL97a}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Greedy algorithms}{9}{subsubsection.2.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Artificial neural networks}{9}{subsubsection.2.2.2}}
\newlabel{sec:machine}{{2.2.2}{9}{Artificial neural networks\relax }{subsubsection.2.2.2}{}}
\citation{citeulike:XuSimple}
\citation{Xu:Transitions}
\citation{Borrett96adaptiveconstraint}
\citation{paper:hybrid}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Hybridisation}{10}{subsubsection.2.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Criteria}{10}{subsection.2.3}}
\newlabel{sec:crit}{{2.3}{10}{Criteria\relax }{subsection.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Accuracy}{10}{subsubsection.2.3.1}}
\citation{Xu:Transitions}
\newlabel{eq:conversion}{{4}{11}{Accuracy\relax }{equation.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Time}{11}{subsubsection.2.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}The data}{11}{subsubsection.2.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}The elephant in the room}{11}{subsection.2.4}}
\citation{complexityZoo}
\citation{Sipser:2005}
\citation{Ed-Karp}
\citation{YE}
\citation{Krak}
\@writefile{toc}{\contentsline {section}{\numberline {3}Discussion of technical literature}{12}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Existing algorithms}{12}{subsection.3.1}}
\newlabel{approx1}{{5}{12}{Existing algorithms\relax }{equation.5}{}}
\citation{SafDinur}
\citation{Sipser:2005}
\citation{citeulike:Karp}
\citation{citeulike:Cook}
\citation{citeulike:XuSimple}
\newlabel{approx2}{{6}{13}{Existing algorithms\relax }{equation.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Annotated bibliography of relevant technical literature}{13}{subsection.3.2}}
\citation{Mitchell.97}
\citation{paper:hybrid}
\citation{PCPLec}
\citation{KhotSurvey}
\@writefile{toc}{\contentsline {section}{\numberline {4}The graph}{15}{section.4}}
\newlabel{sec:theGraph}{{4}{15}{The graph\relax }{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Asymptotic analysis}{16}{subsection.4.1}}
\newlabel{sec:asyGraph}{{4.1}{16}{Asymptotic analysis\relax }{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Vertex (un)marking}{16}{subsubsection.4.1.1}}
\newlabel{eq:timeMarking}{{7}{16}{Vertex (un)marking\relax }{equation.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Vertex visiting and retrieving unmarked edges}{16}{subsubsection.4.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Calculating the cluster coefficient}{17}{subsubsection.4.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Storage}{17}{subsection.4.2}}
\newlabel{sec:graphSave}{{4.2}{17}{Storage\relax }{subsection.4.2}{}}
\citation{Xu:Transitions}
\@writefile{toc}{\contentsline {section}{\numberline {5}Test case generation framework}{18}{section.5}}
\newlabel{sec:tgf}{{5}{18}{Test case generation framework\relax }{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}The training and testing data}{19}{subsection.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Creating statistic datasets}{19}{subsection.5.2}}
\newlabel{sec:stats}{{5.2}{19}{Creating statistic datasets\relax }{subsection.5.2}{}}
\citation{simAnneal}
\@writefile{toc}{\contentsline {section}{\numberline {6}Greedy algorithms}{20}{section.6}}
\newlabel{sec:GREEDY}{{6}{20}{Greedy algorithms\relax }{section.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces An example of a difficult hill to climb.}}{21}{figure.1}}
\newlabel{greedy}{{1}{21}{Greedy algorithms\relax }{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}The base algorithm}{21}{subsection.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Some improvements}{21}{subsection.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The basic greedy algorithm for vertex covering.}}{22}{figure.2}}
\newlabel{baseAlgo}{{2}{22}{The base algorithm\relax }{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces How the post-processing algorithm executes.}}{22}{figure.3}}
\newlabel{fig:uncover}{{3}{22}{Some improvements\relax }{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces How the algorithm may cover a graph like this.}}{23}{figure.4}}
\newlabel{fig:graph1}{{4}{23}{Some improvements\relax }{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces How the algorithm \emph  {should} cover a graph like this.}}{23}{figure.5}}
\newlabel{fig:graph2}{{5}{23}{Some improvements\relax }{figure.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.1}Resulting algorithms}{23}{subsubsection.6.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Testing and analysis}{24}{subsection.6.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}First set}{24}{subsubsection.6.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Results of the algorithms being run on the graphs of set 1.}}{24}{table.1}}
\newlabel{table:res1}{{1}{24}{First set\relax }{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Coverages of each of the algorithms run on the graphs of set 1. Best are bolded.}}{24}{table.2}}
\newlabel{table:res12}{{2}{24}{First set\relax }{table.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.2}Second set}{25}{subsubsection.6.3.2}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Results of the algorithms being run on the graphs of set 2.}}{25}{table.3}}
\newlabel{table:res2}{{3}{25}{Second set\relax }{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The coverage of each algorithm on each graph in set 2. Best are bolded.}}{25}{table.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Aggregate results}{25}{subsection.6.4}}
\newlabel{sec:aggRes}{{6.4}{25}{Aggregate results\relax }{subsection.6.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The coverages of the algorithms on both sets. Lower is better.}}{26}{figure.6}}
\newlabel{fig:greedyResults}{{6}{26}{Aggregate results\relax }{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Time taken in seconds for the algorithms over both sets.}}{26}{figure.7}}
\newlabel{fig:greedyTimes}{{7}{26}{Aggregate results\relax }{figure.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Asymptotic time analysis}{27}{subsection.6.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.1}Base algorithm}{27}{subsubsection.6.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Brief top level view of the longest execution path within the algorithm. The bottom level result of all the if statements are the same: (a vertex get marked and zero or more get visited).}}{27}{figure.8}}
\newlabel{fig:asyBase}{{8}{27}{Base algorithm\relax }{figure.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.2}Preprocessing algorithm}{28}{subsubsection.6.5.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.3}Post-processing algorithm}{28}{subsubsection.6.5.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.4}Putting it all together}{28}{subsubsection.6.5.4}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Artificial neural networks}{29}{section.7}}
\newlabel{sec:ANN}{{7}{29}{Artificial neural networks\relax }{section.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Framework architecture}{29}{subsection.7.1}}
\newlabel{SigFunc}{{12}{29}{Framework architecture\relax }{equation.12}{}}
\citation{Fahlman88anempirical}
\citation{Cascade-correlation}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1}Calculations and training functions}{30}{subsubsection.7.1.1}}
\citation{Cascade-correlation}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}ANN instance design}{31}{subsection.7.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.1}Feature selection}{31}{subsubsection.7.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The structure of the ANN. Not shown is the bias input to each unit from unit -1.}}{32}{figure.9}}
\newlabel{fig:anns}{{9}{32}{Instance topology\relax }{figure.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.2}Instance topology}{32}{subsubsection.7.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Training}{32}{subsection.7.3}}
\newlabel{sec:ANNTrain}{{7.3}{32}{Training\relax }{subsection.7.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The network was trained to Epoch 7255 before it was forcibly terminated.}}{33}{figure.10}}
\newlabel{fig:5Training}{{10}{33}{Training\relax }{figure.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces The absolute error of testing trained networks.}}{33}{table.5}}
\newlabel{table:absErr}{{5}{33}{Training\relax }{table.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Running the network}{34}{subsection.7.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.1}Vertex-wise processing results}{34}{subsubsection.7.4.1}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Cover results for set 1 using vertex-wise processing.}}{34}{table.6}}
\newlabel{table:res1Vert}{{6}{34}{Vertex-wise processing results\relax }{table.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Cover results for set 2 using vertex-wise processing.}}{34}{table.7}}
\newlabel{table:res2Vert}{{7}{34}{Vertex-wise processing results\relax }{table.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.2}Edge-wise processing results}{35}{subsubsection.7.4.2}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Cover results for set 1 using edge-wise processing.}}{35}{table.8}}
\newlabel{table:res1Edge}{{8}{35}{Edge-wise processing results\relax }{table.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Cover results for set 2 using edge-wise processing.}}{35}{table.9}}
\newlabel{table:res2Edge}{{9}{35}{Edge-wise processing results\relax }{table.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.3}Discussion and plots}{35}{subsubsection.7.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Plots of cover against instance for the vertex-wise processing method.}}{36}{figure.11}}
\newlabel{fig:coverPlotsVP}{{11}{36}{Discussion and plots\relax }{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Plots of cover against instance for the edge-wise processing method.}}{36}{figure.12}}
\newlabel{fig:coverPlotsEP}{{12}{36}{Discussion and plots\relax }{figure.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Plots of time taken against instance for the edge-wise processing method. This data was generated on a computer with 8 core i386 Intel Xeon processor running CentOS 5.5.}}{37}{figure.13}}
\newlabel{fig:rtANN}{{13}{37}{Discussion and plots\relax }{figure.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Asymptotic analysis}{37}{subsection.7.5}}
\citation{NPanns}
\citation{Peterson89anew}
\citation{Peterson88neuralnetworks}
\citation{Gallardo_solvingweighted}
\@writefile{toc}{\contentsline {paragraph}{Vertex-wise processing:}{38}{section*.2}}
\@writefile{toc}{\contentsline {paragraph}{Edge-wise processing:}{38}{section*.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6}Improvements}{38}{subsection.7.6}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Hybridisation}{38}{section.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}The algorithm}{39}{subsection.8.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Running}{39}{subsection.8.2}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Results for the Hybrid algorithm being run on set 1.}}{39}{table.10}}
\newlabel{table:Hybrid1}{{10}{39}{Running\relax }{table.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces Results for the Hybrid algorithm being run on set 2.}}{39}{table.11}}
\newlabel{table:Hybrid2}{{11}{39}{Running\relax }{table.11}{}}
\citation{SafDinur}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Comparison plots of percentage coverages for all the greedy algorithms vs the hybrid.}}{40}{figure.14}}
\newlabel{fig:coverPlotsHy}{{14}{40}{Running\relax }{figure.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Asymptotic analysis}{40}{subsection.8.3}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Discussion and conclusions}{40}{section.9}}
\newlabel{sec:results}{{9}{40}{Discussion and conclusions\relax }{section.9}{}}
\citation{Khot}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}The unique games conjecture}{41}{subsection.9.1}}
\newlabel{sec:UGC}{{9.1}{41}{The unique games conjecture\relax }{subsection.9.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Training of the ANN}{41}{subsection.9.2}}
\citation{Xu:Transitions}
\citation{Chickering96lns}
\citation{noNPBayes}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.1}Different models}{42}{subsubsection.9.2.1}}
\citation{Peterson88neuralnetworks}
\citation{Peterson89anew}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Literature review retrospective}{43}{subsection.9.3}}
\newlabel{sec:retro}{{9.3}{43}{Literature review retrospective\relax }{subsection.9.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Further work}{43}{subsection.9.4}}
\newlabel{sec:keepOnGoing!}{{9.4}{43}{Further work\relax }{subsection.9.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5}Personal reflection}{43}{subsection.9.5}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Disc contents and how to use them}{45}{section.A}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Methods of obtaining the code}{45}{subsection.A.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Installing}{45}{subsection.A.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Running the code}{45}{subsection.A.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.3.1}ANN commands}{46}{subsubsection.A.3.1}}
\@writefile{toc}{\contentsline {paragraph}{proofOfConcept:}{46}{section*.4}}
\@writefile{toc}{\contentsline {paragraph}{createANN $<$save name$>$:}{46}{section*.5}}
\@writefile{toc}{\contentsline {paragraph}{displayANN $<$save name$>$:}{46}{section*.6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.3.2}Training commands}{46}{subsubsection.A.3.2}}
\@writefile{toc}{\contentsline {paragraph}{train $<$starting ANN$>$ $<$save file$>$ $<$DIRECTORY of graphs$>$:}{46}{section*.7}}
\citation{Mitchell.97}
\@writefile{toc}{\contentsline {paragraph}{test $<$starting ANN$>$ $<$save file$>$ $<$DIRECTORY of graphs$>$:}{47}{section*.8}}
\@writefile{toc}{\contentsline {paragraph}{cover $<$starting ANN$>$ $<$save file$>$ $<$DIRECTORY of graphs$>$:}{47}{section*.9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.3.3}Other single commands}{47}{subsubsection.A.3.3}}
\@writefile{toc}{\contentsline {paragraph}{generate $<$n$>$ $<$alpha$>$ $<$p$>$ $<$r$>$ $<$save file$>$:}{47}{section*.10}}
\@writefile{toc}{\contentsline {paragraph}{convert $<$DIMACS Graph$>$:}{47}{section*.11}}
\@writefile{toc}{\contentsline {paragraph}{runHybrid $<$graph file$>$:}{47}{section*.12}}
\@writefile{toc}{\contentsline {paragraph}{runGreedy $<$algoNumber [1 .. 4]$>$ $<$graph file$>$:}{47}{section*.13}}
\@writefile{toc}{\contentsline {section}{\numberline {B}A brief primer in artificial neural networks}{47}{section.B}}
\newlabel{primer}{{B}{47}{A brief primer in artificial neural networks\relax }{section.B}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Structure}{47}{subsection.B.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces An example of a unit (Sigmoid).}}{48}{figure.15}}
\newlabel{fig:unit}{{15}{48}{Structure\relax }{figure.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Computing the output}{48}{subsection.B.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Calculating the error}{49}{subsection.B.3}}
\citation{Mitchell.97}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4}Backpropagation}{50}{subsection.B.4}}
\newlabel{backprop}{{B.4}{50}{Backpropagation\relax }{subsection.B.4}{}}
\newlabel{eq:backdiff}{{24}{50}{Backpropagation\relax }{equation.24}{}}
\newlabel{equation:hidden}{{26}{51}{Backpropagation\relax }{equation.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.5}Classification}{51}{subsection.B.5}}
\newlabel{sec:crosse}{{B.5}{51}{Classification\relax }{subsection.B.5}{}}
\citation{PCPLec}
\citation{PCP1}
\citation{PCP2}
\citation{PCP3}
\@writefile{toc}{\contentsline {section}{\numberline {C}The PCP theorem}{52}{section.C}}
\newlabel{sec:PCP}{{C}{52}{The PCP theorem\relax }{section.C}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Dataset plots}{53}{section.D}}
\newlabel{app:DS}{{D}{53}{Dataset plots\relax }{section.D}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Plot of accuracy against p value for $\alpha = 0.2$.}}{53}{figure.16}}
\newlabel{fig:0.2}{{16}{53}{Dataset plots\relax }{figure.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Plots of accuracy against p value for $\alpha = 0.3$.}}{54}{figure.17}}
\newlabel{fig:0.3}{{17}{54}{Dataset plots\relax }{figure.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Plot of accuracy against p value for $\alpha = 0.4$.}}{54}{figure.18}}
\newlabel{fig:0.4}{{18}{54}{Dataset plots\relax }{figure.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Plots of accuracy against p value for $\alpha = 0.5$.}}{55}{figure.19}}
\newlabel{fig:0.5}{{19}{55}{Dataset plots\relax }{figure.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Plot of accuracy against p value for $\alpha = 0.6$.}}{55}{figure.20}}
\newlabel{fig:0.6}{{20}{55}{Dataset plots\relax }{figure.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Plots of accuracy against p value for $\alpha = 0.7$.}}{56}{figure.21}}
\newlabel{fig:0.7}{{21}{56}{Dataset plots\relax }{figure.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Plot of accuracy against p value for $\alpha = 0.8$.}}{56}{figure.22}}
\newlabel{fig:0.8}{{22}{56}{Dataset plots\relax }{figure.22}{}}
\bibstyle{alpha}
\bibdata{citation}
\bibcite{complexityZoo}{AKG11}
\bibcite{PCP1}{ALM{$^{+}$}98}
\bibcite{PCP2}{AS98}
\bibcite{buss:560}{BG93}
\bibcite{citeulike:Karp}{BKMT75}
\bibcite{BAL97a}{BP97}
\bibcite{NPanns}{BR93}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Plots of accuracy against p value for $\alpha = 0.9$.}}{57}{figure.23}}
\newlabel{fig:0.9}{{23}{57}{Dataset plots\relax }{figure.23}{}}
\bibcite{Borrett96adaptiveconstraint}{BTW96}
\bibcite{YE}{BYE81}
\bibcite{Chickering96lns}{Chi96}
\bibcite{Chu36a}{Chu36}
\bibcite{Cascade-correlation}{CL90}
\bibcite{citeulike:Cook}{Coo71}
\bibcite{queens}{Cra92}
\bibcite{stocks}{CT10}
\bibcite{PCP3}{Din06}
\bibcite{noNPBayes}{Doj06}
\bibcite{SafDinur}{DS05}
\bibcite{Ed-Karp}{EK72}
\bibcite{nukes}{EVP72}
\bibcite{Fahlman88anempirical}{Fah88}
\bibcite{paper:hybrid}{FHH{$^{+}$}09}
\bibcite{Gallardo_solvingweighted}{GCF}
\bibcite{Krak}{Kar05}
\bibcite{simAnneal}{KGV83}
\bibcite{Khot}{Kho02}
\bibcite{KhotSurvey}{Kho05}
\bibcite{website:quantum}{Ler10}
\bibcite{citeulike:NASA}{LLH{$^{+}$}03}
\bibcite{Mitchell.97}{Mit97}
\bibcite{PCPLec}{Oâ€05}
\bibcite{Peterson88neuralnetworks}{PA88}
\bibcite{Peterson89anew}{PS89}
\bibcite{Sipser:2005}{Sip05}
\bibcite{citeulike:XuSimple}{XBHL05}
\bibcite{Xu:Transitions}{XL06}
